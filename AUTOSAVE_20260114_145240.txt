[00:00.00]  then have uh as a recourse yeah no so i i think i think that is uh uh an excellent point so um
[00:11.84]  i would make a note to my colleagues who are here with me um to make sure that we do include
[00:17.28]  redress explicitly within the action plan um i think there's a logical place for it so we just
[00:22.08]  have to find it so that's good uh yes yes thank you very good to see you again um yeah i think
[00:29.44]  what ali said actually it will fit there in any case because when we say guardrails i guess it
[00:36.24]  will also involve regulatory stuff but also unavoidably at some point popping liability
[00:42.48]  and as you said we want liability to continue with liability so i guess that may be the place
[00:49.36]  compared to the three parts i see there that it would be also discussed and what ali said
[00:59.44]  and redress and so on i guess these are liability issues and there as we discussed eric some weeks
[01:07.68]  ago of course liability could get into manufacturer's liability into the user's
[01:12.88]  liability and then get a bit more interesting there but uh yeah i think uh regulation
[01:21.68]  liabilities are kind of two sides of the same coin somehow yeah and so uh yes
[01:29.44]  and uh i definitely think like liability obviously i would dare say it's one of the
[01:36.16]  key actions the action plan but we should make sure that we do associated with the redress point
[01:41.04]  that elise was bringing up before i'm going to go to anton and don oh sorry no i was thinking um
[01:58.24]  this is a great opportunity
[01:59.84]  to validate a lot of the questions and discussions we have been working in this
[02:04.32]  group but also most of the experiences that i have in this type of conferences
[02:10.64]  i think is key in um to prepare the the ministers of our countries or the or the
[02:19.60]  or the technical people that that are going to be there so just an idea is to um from this group
[02:28.24]  to create for
[02:29.84]  i mean for the countries we represent uh one pager with key information of what's really going on in
[02:37.92]  the country in terms of i don't know we can define in terms of the ai in terms of digital
[02:43.44]  transformation specific needs and specific questions so that they they can have this as a
[02:50.80]  tool during the conference to to get deeper and to get more from the conference so just an idea you
[02:57.60]  can consider it thank you
[02:59.68]  yeah so uh i like that idea um the the intention of the the attendees the conference is that
[03:08.40]  they're not they're there as experts so they know the content but having said that i do think
[03:18.32]  them being equipped with this is what the lay of the land is locally would certainly help
[03:26.88]  um one of the things that we did uh
[03:29.44]  were considering writing ahead of the conference as a sort of big pre-read for the participants
[03:37.90]  was a scan of the typical barriers that people talk about as reasons why we are unable to
[03:51.04]  responsibly scale AI solutions. Some of those barriers are real, some of them are fictitious,
[03:56.72]  but they're used as rationales and reasons. Because theoretically, each of the actions we
[04:04.60]  have in the action plan should be associated with one of the barriers. That's a theoretical way of
[04:11.04]  thinking of it. So that is one of the other things we were thinking of writing in the next
[04:16.56]  several months, just to make sure that we had a broad perspective. And that might actually be
[04:24.34]  a good thing for me to work on.
[04:26.72]  With this group to make sure that we have perspective on that. That's cool.
[04:31.64]  And we're happy to prepare because, again, it's a great opportunity. And the equity and the
[04:38.94]  challenges of the countries that are going to participate are different. So we have different
[04:43.44]  completely needs in Mexico and Costa Rica than in France and UK and the US. So I think it's a
[04:51.54]  great opportunity to develop what else is needed in terms of
[04:56.72]  the rest of the world. Thank you.
[04:58.40]  Thank you, Anton. Awesome. We'll go with Laura.
[05:02.88]  I want to underscore what Anton just said about the importance of preparation.
[05:09.36]  I think it would be good in terms of pre-work for the country delegates to come knowing
[05:14.24]  where sort of liability sits now. In our circumstance in the US, just as an example,
[05:21.60]  much of it sits with the provider that's using it. And it's still
[05:26.72]  right now sits on their shoulders. And there are some other aspects of where patients might go for
[05:33.70]  redress. But I think having them come with some landscape knowledge of their own situation would
[05:40.36]  be helpful. And many, of course, will come prepared. But I think it might be good if it
[05:43.82]  was systematically requested. I think another suggestion that I would have is oftentimes when
[05:52.48]  I'm participating in liability conversations, people want to identify themselves. And I think
[05:56.72]  it's important to identify the single entity or the group that should be held liable. And I think
[06:01.46]  we're going to have to for a period of time when this is, when we're really discovering and
[06:07.10]  understanding. No one really knows how this plays out. Just thinking about entertaining the idea of
[06:15.10]  shared liability for maybe a period of time. So just that concept. I would also suggest that if
[06:24.62]  we're not already in a situation where we're not already in a situation where we're not already in a situation
[06:26.70]  of
[06:43.88]  whatever, to respond in the same way to maybe help us with generation of truly creative. So
[06:53.08]  we don't come with our own sort of a country views or our own locked in sense of, of where liability
[06:56.64]  something that is truly novel. The last thing I'll just say is I don't know to what extent,
[07:03.12]  since this is focused on what governments should do, we just had a recent announcement last week
[07:09.48]  of the relaxation, so to speak, of it allows those that are developing direct-to-consumer
[07:17.70]  tools to quite significantly skirt our FDA in the U.S. and not necessarily have to go through
[07:27.12]  that process in order to bring things directly to, and this could include things like that might
[07:32.50]  be considered and formerly should have been going through the regulatory process, blood pressure,
[07:40.82]  these types of things that are then going to be incorporated into our clinical records here,
[07:46.26]  but they will not have come.
[07:47.70]  through any formal regulatory. So I think there is that burgeoning issue of things that are sort
[07:54.64]  of allowed to skirt the regulatory framework. And so I don't know to what extent you would be
[08:01.26]  considering that, but it just seems, I know it's going to be material here as we start to watch
[08:06.08]  things like the chatbots that have resulted in some teenage suicides here. Those are direct-to-consumers
[08:12.88]  and heretofore not regulated.
[08:17.16]  Okay.
[08:17.70]  Not just there. The teenage suicides are happening in other parts of the world as well.
[08:23.52]  I think that's where it is good advice for me to broaden what language I'm using in this space,
[08:31.76]  that it's not just direction to government, it's direction. I think there needs to be some
[08:38.10]  language that's reflective of the actions that are taken collaboratively across public sectors,
[08:45.82]  private sectors, and the public.
[08:47.70]  Exactly.
[08:48.52]  That collectively everyone is working together to work on a name, and all three entities are
[08:54.20]  accountable for taking the actions and being involved in and disseminating, et cetera, the
[08:59.20]  actions. So I need to be smarter about, careful about my language, because I think that's an
[09:06.48]  excellent point. Cool. Olga.
[09:09.32]  Thank you so much, Eric. Actually, my point echoes or continues the theme that Laura has brought up.
[09:17.52]  Thank you so much, Eric. Actually, my point echoes or continues the theme that Laura has brought up.
[09:17.70]  about direct to consumer devices, but also the emergence of Dr. Me. For many years, it was Dr.
[09:25.42]  Google that people went to, but now it has gone further than that, and people actually interact
[09:30.96]  with AI in a very sophisticated way, especially recently we've heard about both Anthropic and
[09:36.78]  Chet GPT acquiring specific healthcare capability through new acquisitions. So there'll be more
[09:43.24]  really high-level, sophisticated interactions between consumers,
[09:47.70]  and AI on that. And I think the notion of accountability and liability and kind of
[09:57.14]  safeguards in this area is also very important. And just to express very strong support for the
[10:06.90]  clarity around medical, legal liability for healthcare professionals,
[10:11.54]  I've mentioned before the Nuffield Trust report, how GPs use AI in the UK,
[10:17.54]  Dr. Me. for healthcare professionals. I've mentioned before the Nuffield Trust report, how GPs use AI in the UK, the
[10:17.86]  than our company supported. And it's absolutely fascinating that the main
[10:23.26]  barrier for adoption of AI tools, both for doctors who use and those who do not use AI,
[10:29.00]  is the fact that they don't really know who is responsible for any – if anything goes wrong because
[10:35.08]  there is lack of clarity for this in the UK and I suspect in other places as well. So overcoming this
[10:41.72]  would absolutely be something that will facilitate acceleration of AI. I think it's been important,
[10:47.64]  in healthcare. Thank you. Great. Great. I wonder if that's a general
[10:54.98]  lens we should be prepared to take on the overall session is the whole doctor meme, so to speak,
[11:04.82]  because literally in the last two, three weeks, I keep on getting bombarded with,
[11:12.26]  oh, look, more or less all the big tech guys have done their health-specific version of their
[11:19.80]  LLN. And so understanding that that is where the world is now and how do we respond to that
[11:30.96]  effectively, I think is one of the important things to think about. Yeah. Cool. Yvonne?
[11:40.24]  Happy New Year, Eric. Happy New Year.
[11:42.26]  Happy New Year, everybody. Yes. So I'd like to start by saying, yes, I agree with what Elise has
[11:51.14]  said about needing to talk about recourse and also with Laura about shared liability. And the word
[12:00.14]  shared is the operative word here. I think, and Eric, correct me if I'm wrong, but as the only
[12:09.64]  physician here in this call,
[12:12.26]  while we're talking here, I think it's really important to advocate for humanity.
[12:21.30]  AI is not coming to this conference. It does not actually have a seat on the table. It is humans
[12:28.18]  here. And we are as humans, it's a human conference. So I think we need to ensure that we
[12:35.96]  are doing the absolute best for the humans. And I'll elaborate a little bit more as I keep going.
[12:43.12]  Sifeng Duu workanca de ora?!
[12:43.16]  I think that's Нов data is fine. Yeah. All right. Then I'll continue sharing with you that we have
[12:47.18]  some excellent some excellent things so far. So we talked about things related to liability, which is
[12:54.26]  kind of like in a way if you like it's kind of like the negative face of what we're talking
[12:59.12]  about when things go wrong. And yes, I totally agree, and things will go wrong. A few things
[13:05.36]  before I get on to what we could also do as a positive thing for the conference, is I think it needs to be a little bit more, in addition to guardrails a little bit more in addition to humans in the house.
[13:12.26]  loop because for example humans can be the loop but the human may not have all the um perspective
[13:21.78]  because that human who is in the loop may not have been the person who invented who developed the ai
[13:30.74]  so what am i talking about being positive so in addition to recourse in addition to
[13:36.18]  shared liability i think it's really important to have in the conference
[13:41.30]  to ensure that we talk about the four pillars of by ethics so the things to highlight beneficence
[13:52.42]  non-maleficence in other words do no harm so all the ai that come into fruition
[14:00.66]  need to pass this test beneficence non-maleficence autonomy which
[14:06.18]  means that if the patient should choose not to have ai in their care they should have the right
[14:13.62]  to do it and have equal care and justice which means that it doesn't really matter who you are
[14:19.14]  there's equitability um because we need that for the world and i think it's really really important
[14:28.90]  that this becomes a topic in addition to recourse which i think is really important and in addition
[14:36.18]  to shared liability thank you one one the doubling back of on these have four parts uh and i wrote
[14:45.14]  down three uh uh oh so beneficence non-maleficence autonomy and rachel's bespoke justice okay cool um
[14:57.46]  one of the other things i was thinking of um obviously i have now moved on to talking about
[15:02.90]  the action plan itself but um
[15:07.14]  i'll reflect it directly in a minute one of the other things that i was thinking of doing
[15:14.58]  in parallel in the next few months is a small write-up that because intentionally i'm trying
[15:22.58]  to get the um two pages to be easily consumable as easily consumable as i can but at the same time
[15:32.26]  to make it consumable you lose some of the nuance and so
[15:36.18]  my what i was thinking was to have a slightly longer right up around each of the specific
[15:44.34]  sub-actions that are on the second page and that's where i would bring in exactly the points that you
[15:50.34]  were just talking about uh avon so i think i where i was thinking about at the back of my head and
[15:57.22]  now thinking it needs to come to front of my head uh in terms of something that's important to do
[16:03.70]  so that was me just consuming thinking and having a take-home from the first two slides um i i think because of course i wanted my papers to be accessible to anybody at the webinar panel um had the fashion Ley, you've gone through all the modules and have numbers ever since and um it's it's let's say a lot of passes and points but you've been dealing with Theater Committee, Badians, R Sony and other indications and uh i i i think that
[16:06.18]  about that oh sorry eric so i forgot one more thing that i wanted to add we've always talked
[16:12.30]  for for the whole um you know for the year and so on all the time we've been here about
[16:16.84]  improving trust and having this um the bioethics part in this would improve trust because we you
[16:27.10]  know we have nothing against ai we just want to make sure that it is good but we have everything
[16:33.32]  against bad ai and all of us here also don't want bad ai so all all this the the bioethics part
[16:42.10]  will also contribute towards trust and moving everything forward as as you have outlined to
[16:48.72]  us terry thank you so uh with that said i will move on to to reflect in the action plan
[16:57.78]  and directly i'll i'll reflect your comment
[17:02.36]  because
[17:03.32]  in the action plan as i redrafted it and thank you all for the feedback that i received on it
[17:10.48]  between december and january um uh one of the important things that i was able to do in the
[17:18.48]  action plan was i moved this section on earned trust from the more or less the the rightmost
[17:26.62]  section to being the leftmost in the action plan itself being at the bottom part of the page to the
[17:32.32]  top half of the page
[17:33.32]  to the point now where the first line of the action plan the detailed action plan and is
[17:40.18]  embed ethics and respect for human rights and require the ai system use to demonstrate positive
[17:46.32]  benefit for patients in the public so very much trying to get to that point with us uh so all of
[17:54.18]  that said avon strong agreement and bluntly avon knows that was her specific suggestion
[18:03.32]  of b iPod therefore ot follower is a possibility yeah without external
[18:26.16]  often
[18:27.00]  and but so yes i embraced that suggestion um entered the action plan itself because it made a lot of sense to me because a lot of
[18:28.00]  part of what we'veást identifying within this form in the span of the last year and a half has been how do we reflect that doing ai and health Necessarily involved a lens of ethics so hence why putting it up front making it clear i think is one of the strong things about research and human health of ethics is
[18:33.32]  we can do okay um so let me segue for a bit so let me talk a little bit about the action plan itself
[18:45.56]  um some of you so uh sarah uh attached the updated version of the action plan
[18:53.16]  earlier this week so it's on the invitation for this meeting um i'm not going to go through it in
[18:59.56]  depth right now but i wanted to touch on some of the aspects of it i do invite that if people
[19:11.16]  do want to provide additional feedback on the action plan um i'm still very happy to receive
[19:17.80]  that in the next few months um and because of the how this conversation has gone it's actually
[19:23.88]  changed some of my idea around what some of our next steps are um
[19:29.80]  cool um well i'll just take leah's suggestion as being good and trust that somebody is recording um
[19:39.80]  so uh cool the um so the action plan itself the um you uh back in our september meeting we actually
[19:53.64]  did have an earlier version of the first page of this plan um i had started to develop the
[19:59.80]  with some uh colleagues uh in march of last year um and uh but that's why it's also great to have
[20:09.64]  received such good robust feedback all the stuff that you see in the action plan in red
[20:15.00]  is just simply reflections of where i've changed or added words um because of the review of this
[20:23.64]  group and as you can see by the sheer amount of bread on the document there was a lot of very good
[20:29.56]  feedback um the the general feedback was the uh a lot of the the action plan itself covered a lot
[20:38.36]  of the ground that needed to be covered but some of the words or focus areas within the action plan
[20:44.20]  itself could have been could be strengthened and that's why i think that is or at least i
[20:49.00]  hope that is what we have managed to achieve uh with us over the last of the water um
[20:55.40]  also emphasizing the importance of partnership and cross-roads for an
[20:59.56]  the public sector, the private sector, and the public toward a common aim.
[21:05.50]  And importantly, and I think I might even strengthen reflective of our earlier conversation
[21:12.72]  about why cooperative action matters now is, in a longer write-up, I would certainly reflect
[21:22.44]  that, listen, there are dozens of these LLM health-specific things that are being put
[21:28.40]  out there, and people are using them.
[21:31.28]  There was a study back in last month that, what was it, 40% of people, or no, one-third
[21:46.32]  of people believe that they can understand their own health as good as a doctor's advice
[21:53.66]  can give them by using the internet after one week.
[21:58.40]  So one week, and I'm as good as a doctor.
[22:01.68]  That was, yeah, I certainly agree with this, the oh, no thing.
[22:06.72]  But sadly, now, in the same survey, ironically, or maybe happily, 40% of people reported regretting
[22:17.72]  making a decision and taking action based on advice they gleaned from the internet without
[22:24.40]  advice from a doctor.
[22:27.06]  So good news.
[22:29.04]  People actually do respect and value the advice they get from the medical system, from the
[22:34.74]  health system itself.
[22:36.42]  Bad news.
[22:38.96]  They still think that they can actually do it themselves.
[22:41.70]  So, and they have been led to believe that they could probably achieve good outcomes
[22:47.74]  themselves.
[22:48.64]  And I don't know if folks heard, some of the work of AI has started to become semi-autonomous,
[22:55.68]  where in Utah...
[22:58.40]  In Utah there was approval for AI to provide prescription renewals for people without intervention
[23:07.60]  of a medical provider.
[23:09.06]  I'm sorry, I'm just seeing the expression of your face, Clara.
[23:17.14]  I'm sorry.
[23:18.94]  Yeah, sadly, yeah.
[23:22.78]  So I mean, things are happening and happening fast.
[23:27.04]  I don't know.
[23:28.04]  I don't know where we thought the world would be one year ago, but if we project forward
[23:34.02]  to where the world is going to be a year from now, it's not going to be a linear exercise
[23:39.16]  between where it was a year ago and to today.
[23:42.02]  It will be an exponential exercise.
[23:44.64]  So if we think things have gotten better by two, they will get better by another factor
[23:49.70]  of four from here, or different by a factor of four, and that's really better, which is
[23:55.62]  exactly the point that we're having.
[23:58.74]  So the point of the action plan, as you can see, at the high level for the first page,
[24:10.40]  it really reflects on the need to grounding in an earning of trust, which reflects what
[24:18.40]  I've heard from this group time and time again, and what Yvonne said earlier on in her
[24:22.80]  intervention.
[24:24.86]  Then we want to make sure that we do enable what works.
[24:28.04]  But as we are enabling what works, we need to make sure that we are preventing what works.
[24:33.18]  And so within that, the lens of embedding bioethics and respect for human rights, making
[24:41.74]  sure things are transparent, explainable, not sure if we need to expand that area a
[24:47.18]  little bit, and I'm not sure if I have engaging patient providers in the public, that might
[24:56.32]  be something that we want to...
[24:58.04]  I'm not sure if I have engaging patient providers in the public, that might be something that we
[24:58.54]  want to strengthen, as opposed to just engage, but actually be tempted to change that to
[25:04.38]  involve, because engage, you can do that, but that actually taking into account their
[25:12.32]  considerations.
[25:14.90]  And then the last part about expanding education, just to make sure that people understand what
[25:19.70]  the heck is going on.
[25:21.64]  So that's certainly one thing I would think of from the earning trust bucket.
[25:24.98]  Thank you.
[25:28.04]  I'm going to pause for a sec and say, if I are not going to pause, let me summarize
[25:34.90]  the earning trust bucket and see if there's feedback on it, because so the earning trust
[25:42.64]  really is, if I can paraphrase what's there, put human first, respect ethics, make sure
[25:52.38]  that you're doing things transparently and openly so people understand.
[25:58.04]  We involve all the relevant stakeholders in the overall work, and we make sure people
[26:05.68]  understand what is happening in their system, that they have the tools and the knowledge
[26:10.76]  in order to properly engage with this new AI system, right from the public, right through
[26:17.02]  providers and the support staff, et cetera.
[26:22.44]  And there should probably be something about capacity building in there, too.
[26:25.74]  So I want to say thank you.
[26:26.74]  Thank you.
[26:27.74]  Thank you.
[26:28.74]  Thank you.
[26:29.74]  So when I think of trust, if we have that, the ethics, the human aspects, being transparent,
[26:35.10]  involving the public and making sure people have the skills and knowledge in order to
[26:38.64]  do this work, is there anything massively missing in that bucket?
[26:43.18]  I see Leah.
[26:44.72]  My hand is not for anything that's massively missing.
[26:50.36]  I put it up before you asked the question.
[26:53.38]  No, that's fine.
[26:55.02]  It's fine.
[26:56.02]  It's fine.
[26:57.02]  a couple of points here where there would be benefit of cross-border collaboration it might
[27:04.80]  be helpful if you kind of level set what's the bar by which you mean collaboration because i
[27:10.94]  think you said earlier it's not necessarily sort of like learning from each other it sounds like
[27:15.72]  it might be a more intensive cross-border collaboration but anyhow um for some of these
[27:21.94]  points like number two uh that might be an area where cross-border collaboration might benefit i
[27:29.78]  know there's a lot of struggle around thinking about things like consent and what does that
[27:33.34]  look like how do you do it whatever so um but yeah so that that was my one um piece and particularly
[27:38.86]  number two but if once i have a better sense of sort of where the bar is for what's meant by
[27:43.50]  collaboration it would help me just think through are there certain points here where
[27:47.60]  multi-jurisdictional collaboration might be a benefit
[27:52.08]  when i think of that bar i'll just say my thoughts and others can come way into um and is uh
[28:01.52]  these are areas that by setting um some consistent guideline it means that the
[28:10.82]  implementation and development of ai solutions is uh faster more trusted more effective
[28:18.58]  um so explicitly out of that first bucket
[28:21.88]  um so explicitly out of that first bucket
[28:21.90]  um so explicitly out of that first bucket
[28:21.92]  before, I would explicitly highlight that same one that you just
[28:25.78]  highlighted, because I think that is something that there is benefit in
[28:29.88]  multiple entities agreeing what is sufficiency for
[28:33.84]  transparency and explainability, because that sends a clear signal to the development
[28:38.02]  community about what they have to do in order to make things
[28:41.48]  explainable.
[28:46.30]  Whereas I can educate however I want, I can learn from
[28:49.82]  other countries, but it doesn't need to be done the same. The content
[28:53.38]  would naturally be similar, but how it is done,
[28:58.04]  yeah, I don't think countries need to be doing it the same.
[29:01.92]  Whereas guidance around transparency and explainability,
[29:06.08]  that helps reduce cost burden
[29:09.90]  by being similar or compatible.
[29:13.36]  Great, yeah, thank you very much.
[29:15.08]  Thank you, Lynn.
[29:19.82]  Thank you for this. I was just going to say, Eric and team,
[29:25.10]  it must have been very hard to try to squash it all in, right?
[29:29.04]  You've done a really good job. Thank you.
[29:32.88]  Thank you. Seriously, I mean, obviously, you know, I sent
[29:36.58]  stuff back to you all. Mine wasn't very good, but we all agree on the
[29:40.70]  points. What I wanted to say was that on, you
[29:44.76]  know, the first one that says embed ethics,
[29:47.42]  the action plan, I wonder, and I'm happy to kind of like be guided by you all as well,
[29:55.36]  whether it should be bioethics or somehow, so it's
[29:59.00]  human ethics. It needs to be very clear. It's ethics
[30:03.52]  for the humans. So generally, we use the word
[30:07.52]  bioethics rather than just ethics because, you
[30:11.80]  know, it can be reinterpreted a different way if you just put the word ethics.
[30:17.42]  It could be ethics towards AI. And we,
[30:21.42]  you know, we want to put humans first, which is what we are talking
[30:25.34]  about here. So just make sure, you know, my suggestion would be bioethics,
[30:30.10]  but feel free to kind of like, you know, think about something
[30:33.44]  else, but it can't just be ethics. That's one thing. The other thing is
[30:37.34]  about the words beneficence
[30:41.26]  and non-maleficence because they mean something specific.
[30:47.42]  I don't know how you're going to put it in, but I think it's really important.
[30:51.68]  And the very, very last thing is I can see you've got the demonstrate
[30:55.42]  positive benefits for patients. And I'm also
[30:59.42]  guided by you all as to whether there's got, because
[31:03.14]  people will argue, well, there is positive
[31:07.58]  benefit and there could be like a thousand other negative benefits,
[31:11.50]  but they go, well, you know, I have shown positive benefit. So
[31:15.56]  I wonder whether it
[31:17.42]  could be something like
[31:19.14]  it's a, there are somehow
[31:25.46]  do this, but more positive benefits
[31:29.26]  than negative outcomes. So you
[31:33.70]  have to have a positive, what we call positive
[31:37.38]  benefit risk balance. This is what we say in
[31:41.54]  the Australian system. But it has to be
[31:45.78]  more positive.
[31:47.42]  Not just positive has been more positive than negative.
[31:51.62]  So what we use is positive risk balance.
[32:00.62]  Oh, I like that very much. Thank you.
[32:04.58]  And you're already by your point around bioethics earlier on.
[32:08.74]  So it's in there right now and we'll see how it goes.
[32:11.98]  But for now, I've changed it to bioethics by actually very much like your,
[32:16.94]  your statement, because, well, it's positive over here.
[32:20.24]  Ignore all that stuff, you know, like that.
[32:23.18]  When you think about climate change, hey, we've had great positive benefits.
[32:26.66]  We killed the planet, but great benefits.
[32:28.82]  So hence why bringing in the yin and yang, that risk balance aspect,
[32:34.10]  I think is really, really quite practical.
[32:36.56]  And because it's what we're trying to do.
[32:39.26]  Well, so in turn, let me quickly go through.
[32:42.94]  So in turn, let me quickly go through.
[32:44.94]  So in turn, let me quickly go through.
[32:45.94]  So in turn, let me quickly go through.
[32:46.94]  So in turn, let me quickly go through.
[32:48.50]  The second bucket.
[32:50.62]  So in this where we talk about enabling what works,
[32:56.58]  focus here is around strengthening our data infrastructure,
[33:02.26]  making sure that we have good ways from a lifecycle perspective of evaluating
[33:08.10]  and assuring that AI solutions are effective once they have been approved,
[33:13.46]  that we can effectively and accelerate the integration of those
[33:16.90]  solutions into care practices and workflows and scale those effectively.
[33:22.74]  And then we want to make sure all of that within an environment where our workforce
[33:26.92]  has the capacity and capability to utilize these tools that are being created.
[33:34.44]  So that was our focus is around enabling what works, which in a way is reflective of the
[33:40.20]  overall lifecycle.
[33:42.04]  Make sure your data foundations are strong.
[33:44.78]  Make sure that you can improve the systems.
[33:46.90]  Make sure you can scale and monitor the systems.
[33:51.20]  Make sure that we can use the systems effectively.
[33:54.14]  It's always fun doing this and summarizing it in as few words as I possibly can.
[34:01.30]  So that was the ethos around the enable what works.
[34:05.04]  Does that sound reasonable?
[34:07.04]  What did you just say, Anton?
[34:10.86]  Yeah.
[34:11.86]  Yeah.
[34:12.86]  Yeah.
[34:13.86]  Yeah.
[34:14.86]  Yeah.
[34:15.86]  Yeah.
[34:16.86]  Yeah.
[34:17.86]  That's very true.
[34:18.86]  This is where using AI, bringing a lot of technology to rural and remote communities,
[34:29.62]  while it has risks associated with it, then is it more risky to have an AI solution there
[34:37.94]  than to have nothing there?
[34:40.84]  And if you asked me three years ago, the answer would be yeah, clearly, whereas as in it's
[34:46.26]  more risky to use AI than nothing.
[34:47.26]  But now the question is, there is a risk balance, if I think about Yvonne's words from a few
[34:54.70]  minutes ago, to really think about that.
[34:57.76]  So that I think it really, I wonder if there's an exercise in all this that really reflects
[35:04.40]  on what some of the risk balance areas are.
[35:07.74]  That's something for us to think about, especially what some of the key use cases are that we
[35:12.26]  should be prioritizing.
[35:13.68]  So there's the action plan we should prioritize.
[35:15.26]  Ah, that's something else we could prioritize, is the actual use cases that could be where
[35:24.96]  AI can be deployed, where we should focus our efforts.
[35:28.26]  That's cool.
[35:29.26]  Janice.
[35:30.26]  Yes.
[35:31.26]  Actually, following up with what Anton says there, yeah, indeed, that's another interesting
[35:39.38]  issue there, I think, which could also get into enabling.
[35:43.26]  How?
[35:44.26]  Maybe.
[35:45.26]  Two things there, I think.
[35:47.04]  One of them is how an AI system is also performing in diverse environments.
[35:56.08]  So maybe in rural areas or in different environments.
[36:00.46]  Because it might be FDA or CE marked approved, but maybe for different types of reasons,
[36:08.38]  because what FDA or CE marking provides in Europe, it would be that it's performing technically
[36:14.34]  okay.
[36:15.26]  Under those conditions, parameters, and so on.
[36:19.04]  But it could be the case that it performs differently in different environments, depending
[36:28.26]  on number of factors.
[36:30.28]  And I think also in rural areas or different areas, different types of people and so on,
[36:36.16]  not only on data of people, but also even clinical workflows, for example, could be
[36:41.28]  affecting a bit the education of the physician.
[36:43.04]  The interaction of the physician.
[36:44.04]  Yeah.
[36:45.04]  The interaction, for example, of how physicians interact with AI, how they're assessing the
[36:49.70]  AI system, how they're applying it, and so on, compared, let's say, to a university hospital
[36:56.56]  where a professor is using it, and so on.
[36:59.04]  So I think all these, indeed, are factors there.
[37:03.74]  And secondly, indeed, also, when it comes to rural areas and so on, maybe that's something
[37:11.64]  we should think of the value of AI.
[37:14.62]  AI in remote areas.
[37:16.80]  Because that will also give us, I think, the idea of equitable, if I may call it under
[37:23.22]  the heading of equitable enjoyment of AI by society at large.
[37:30.22]  So we don't create another digital divide between, now as we had in the past, for example,
[37:37.62]  digital divide on other stuff, now we don't create another new AI divide between those
[37:44.26]  who can enjoy AI in metropolitan cities, for example, in big university hospitals
[37:49.28]  and so on.
[37:50.58]  And we leave other people in rural areas or in other countries behind from this enjoyment
[37:59.76]  and revolution.
[38:01.72]  So I think, yeah, these are kind of interesting aspects, I think, that we could also see.
[38:07.44]  Thank you.
[38:08.94]  Yeah.
[38:09.94]  Thank you, Ernest.
[38:11.44]  I'm thinking about...
[38:12.44]  Yeah.
[38:13.44]  Yeah.
[38:14.26]  So I'm thinking through how to incorporate that into what we're thinking in terms of
[38:21.44]  the agenda.
[38:22.44]  I think that maybe there's even...
[38:24.88]  Maybe there's actually a special section that I was thinking about, given all the actions
[38:28.68]  that we've talked about here, what are the use cases, where there is benefit, and which
[38:34.36]  actions benefit which use cases.
[38:36.50]  I wonder if there's something crazy like that.
[38:42.64]  So I...
[38:43.64]  Let me do some thinking about that.
[38:46.72]  And for what it's worth, Janos, I have been using...
[38:50.58]  I had started using a line a year ago about we're at risk of digital divides expanding
[38:59.02]  into digital chasms because of AI.
[39:03.86]  So we want to make sure that if we don't...
[39:07.54]  And the digital chasm is untraversable.
[39:10.98]  So digital divide...
[39:11.82]  Arguably, you can still talk about it.
[39:12.82]  Arguably, you can still build a bridge.
[39:15.90]  Chasm, it's a lot harder.
[39:17.36]  And that is the path that we're at risk of going down.
[39:22.54]  Cool.
[39:23.54]  Laura.
[39:24.54]  Eric, I think you're onto something with this use case.
[39:30.24]  This feels...
[39:31.24]  It's resonating with me because we have thought about that here too at the National Academy.
[39:37.14]  And so one of the initiatives we're launching on March 3rd is to build...
[39:39.90]  I'm sorry.
[39:40.90]  I'm sorry.
[39:41.90]  I'm sorry.
[39:42.82]  I'm sorry.
[39:43.82]  It's a significant one.
[39:44.82]  And it's in the US.
[39:45.82]  I'm sure in Canada, it's hard to say, but it's in the US and the US, there's a lot of
[39:46.82]  organizations that are building these best practices.
[39:47.82]  And I think the first thing we're doing here is to build a first-ever national strategy
[39:48.82]  for patient safety, AI-enabled patient safety.
[39:49.82]  And this is not to be another effort to make AI safe.
[39:54.00]  It's an effort to deploy AI to make care safe.
[39:58.18]  But de facto, we'll be using it for measurement of harm.
[40:01.74]  And that would be harm from all sources, including AI.
[40:05.12]  So it's almost a Trojan horse, if you will.
[40:07.54]  A way to get more of the attention on making AI safe.
[40:08.54]  And that's another effort.
[40:09.54]  What are some of the initiatives that you see as some of the most important?
[40:10.54]  I mean, this is ación.
[40:11.54]  on making AI safe without explicitly going directly at that, which some perceive as
[40:17.92]  slowing down progress. So, our first ever national strategy will be patient safety at scale,
[40:24.62]  meaning what parsimonious four to five actions, four to five areas where we take
[40:31.88]  one to two actions. It's not going to be a 40-point strategy with 25 bullets under each.
[40:37.66]  It's going to be the highest leverage actions we can take to enable every delivery system in the
[40:44.74]  U.S. to perform better on safety using AI as a means to doing that. And the reception that we
[40:52.80]  are getting, people have said, this is the way to break through. This is an area where it's
[40:59.32]  unassailable. Nobody stands up and says, we don't care about safety. In fact, people,
[41:04.92]  you would never stand up.
[41:07.66]  And say, this is not important. It's primacy, really. Do no harm is the first promise.
[41:15.90]  And if we can't make that promise, then I don't know about the rest of them.
[41:19.22]  So, I just support this idea that you're thinking about. And this is not necessarily,
[41:23.58]  I'm saying use the safety use case. But I think this idea of bringing things through in a single
[41:30.16]  more bounded use case makes it more doable. And especially choosing a use case that
[41:35.84]  is defensible.
[41:37.66]  To the max.
[41:40.76]  Yeah.
[41:41.62]  And for what it's worth, I like the safety use case.
[41:45.86]  So, thank you.
[41:49.06]  There are two angles I would bring into that, Laura.
[41:52.64]  One is, I've been working with a researcher out of Alberta around data-related harms.
[41:59.94]  And reflecting that sometimes there are harms that come from the non-use of data as much as
[42:06.26]  there are harms that come from the use of data.
[42:07.66]  data. So we actually have to balance it back and forth. The other report that we actually came up
[42:14.34]  with as the OECD back in March last year was around diagnostic safety. And it was in that
[42:22.76]  report that reflected that 15% of healthcare encounters have a diagnostic error, which
[42:30.70]  contributes to 17.5% of health system costs. And so that, in a way, that very much ties to the
[42:40.60]  safety train. So it's something that we as an organization actually have something to say about.
[42:46.22]  But I think it also, as you said, resonates with whomever you're talking to very, very, very easily.
[42:53.88]  Exactly. It's like a diamond, and you can shine many different facets of that diamond. You can
[42:58.82]  point it toward the sun. So if it's a diamond, you can point it toward the sun. So if it's a
[43:00.68]  someone who's concerned about waste in the system and expenditures that don't have to,
[43:06.70]  especially if you're looking at total cost of care. So they may not feel that moral pull
[43:12.10]  that others feel. They're looking at an economic fine. If that's the door that they want to enter
[43:18.26]  in this, then. But I think it's really an opportunity to do a lot with AI and get it
[43:25.18]  scaling in the name of something absolutely essential to do.
[43:30.68]  Yeah. And it's no longer in the pursuit of a shiny object. It's actually in the pursuit
[43:35.46]  of something we can all agree makes sense. So good. Okay. I definitely have more. Well,
[43:43.76]  my brain is going, as it often does during these calls. Thank you.
[43:49.14]  Um, I think actually very, very helpful. So let me just go through the last column.
[44:00.68]  Just to call them out. So in the context of preventing harm, um, we reflect the, the need
[44:08.74]  to protect, uh, privacy and security while preserving human oversight, making sure we have
[44:15.58]  safeguards after the solutions have been approved. So the enable bucket more or less approves the AI
[44:21.86]  systems and put them onto the market. The prevent harm, uh, bucket makes sure that we are monitoring
[44:28.56]  the systems that both before and after the system has been approved. So we're making sure that we're
[44:29.76]  making sure that we're monitoring the systems that both before and after the system has been approved.
[44:30.68]  and after it is implemented that there are safeguards in place to make sure what is on the market is safe and continues to be safe.
[44:40.72]  And then the last thing is that there are methods to learn from each other as we're doing this work
[44:49.64]  because this is harm that is experienced in one part of the system is an opportunity to prevent harm in another part of the system.
[45:00.28]  So, we want to make sure there are those channels for continuous learning across the overall ecosystem.
[45:08.76]  And so, I'm very much leaning based on what we've been talking about.
[45:13.90]  If I really go to this area and think about diagnostic safety or not diagnostic, that would be too narrow
[45:21.36]  because coming back out that there's some administrative work that is happening.
[45:30.28]  But how do we make sure that these are the use cases where safety is paramount at all cases?
[45:36.78]  Sure.
[45:37.42]  Yeah.
[45:38.06]  Continuing of care.
[45:39.32]  Yeah.
[45:39.68]  Continuing of care.
[45:41.26]  There we go.
[45:42.54]  Cool.
[45:43.18]  Okay.
[45:43.46]  Well, I definitely think that that has been as much as it's been a fast action curve
[45:53.30]  because I think what I'm hearing from folks, apart from the words that are on the page,
[45:59.68]  what would...
[46:00.20]  Yeah.
[46:00.28]  ...really help is, ironically, and correct me if I'm interpreting this wrong,
[46:09.50]  but what I'm hearing folks say is it would benefit this if there were a different page two,
[46:18.26]  which really reflected here is the key use case, which would be related to safety.
[46:26.70]  And in order for that use case to happen,
[46:30.20]  these are the actions that we need to see play out.
[46:35.34]  So really focus on it.
[46:37.12]  Then we annex that.
[46:39.60]  And here is a list of all the actions that were identified through the work that we did with experts.
[46:44.20]  So we really narrow in the focus.
[46:47.74]  Instead of having 36 actions, we'll have the, I'll call it 10.
[46:52.96]  Yeah.
[46:53.34]  Who knows?
[46:54.52]  That would say these are the actions that are most important to be taken in order to assure safety.
[47:00.20]  Is that what I'm hearing?
[47:04.58]  Well, it's what I'm hearing.
[47:06.02]  Is that what I should have heard?
[47:14.02]  For me, when I was talking about safety, I didn't mean, again, focusing directly on the safety of AI,
[47:21.58]  but using AI to make care safe, which puts everyone in the...
[47:27.38]  Obviously, there's an aspect of that that says,
[47:30.20]  and the AI can't harm patients either,
[47:32.84]  but it comes in store with these other things that are...
[47:39.20]  So it isn't perceived as much as people trying to slow down progress.
[47:43.70]  So we're really coming at it from this idea of the actual making care safe using AI,
[47:51.56]  using it as the positive tool that it can be.
[47:54.26]  Yeah, that which is different than what I've been talking about.
[48:00.20]  That is still very related to what I've been talking about.
[48:04.00]  Okay.
[48:04.66]  So let me get my head around this.
[48:08.04]  I'm not exactly sure where we'd go.
[48:13.48]  Use case all to help us focus on the action.
[48:15.68]  Yeah, absolutely.
[48:18.08]  Yeah.
[48:18.56]  So I like the idea of...
[48:23.40]  I'm starting to enjoy the idea of having a second page in here that really says,
[48:27.88]  here is a use case.
[48:30.20]  These are the actions that need to happen for that use case to be live.
[48:34.58]  Here is another use case.
[48:36.26]  So we have a couple of sample use cases that people can chew on.
[48:42.08]  And then we have...
[48:43.46]  Here is a complete list of all the actions.
[48:46.34]  And because then I can see a table that is me being too geeky.
[48:56.98]  Here are the use cases.
[48:57.96]  Here are the actions.
[48:58.76]  And which actions benefit most?
[49:00.20]  And which use cases...
[49:01.34]  Sorry.
[49:01.74]  Which use cases benefit most from which actions?
[49:04.52]  Then you can say, here are the highest value actions to take.
[49:07.72]  Because they actually, by doing this one action, it benefits all use cases or optimal set of use cases.
[49:13.76]  That is something that is missing today.
[49:16.96]  Cool.
[49:17.68]  Sorry.
[49:17.92]  Following the conference, can we develop an agent tailored for government use?
[49:25.22]  Well, I don't think we would.
[49:30.20]  Oh, Asia will also offer best practices to case studies in OECD countries.
[49:34.84]  Well, maybe we would.
[49:36.46]  So thank you for that, Anton.
[49:37.86]  That's interesting.
[49:38.62]  I'm going to talk to my colleagues over in STI.
[49:43.94]  So they're worrying about the overall lens of AI and policy associated with it.
[49:56.54]  They actually created a policy repository over the span of the last...
[50:00.20]  The last six years.
[50:02.06]  And so it would be interesting to talk to them to say how...
[50:06.10]  Because what you suggested is a way they could operationalize that inventory in and of itself.
[50:11.42]  Yeah, that's really interesting.
[50:13.14]  Cool.
[50:14.26]  Okay.
[50:16.86]  So this is the part where I changed what my mind was up front.
[50:26.46]  Because, you know, I've been threatening for...
[50:30.20]  For a couple of times that, you know, this group is going to wind down at some point in time.
[50:36.48]  Just have to admit that this group is going to wind down.
[50:40.00]  I had expected that this would be among the last meetings that we would have.
[50:46.30]  But based on this discussion today, I'm now thinking it'd probably make sense to do one more kick at the can again.
[50:52.72]  Sarah's laughing at me because I keep on doing this every time.
[50:56.40]  But specifically, I think that some of the...
[50:59.20]  Some of the...
[51:00.20]  I think having...
[51:02.18]  There are two...
[51:02.80]  There are two meetings that I think make sense to do.
[51:06.14]  One, which is one prior to the conference that looks at what materials we have.
[51:11.14]  We're developing for the conference to make sure that we aren't having accidental blind spots around that.
[51:17.18]  Especially around the...
[51:19.18]  Our thinking around the actions, the evolution of the action plan as it's gone through several iterations in the next few months.
[51:28.20]  I think that would be incredibly beneficial.
[51:30.20]  That I would suggest is probably going to be in mid-April.
[51:36.34]  I would then suggest that we have...
[51:40.18]  We would set up another meeting with this group in, let's say, mid-June, shortly after the conference,
[51:48.82]  for those who aren't able to otherwise attend the conference because of the constraints that I talked about before,
[51:54.62]  to get feedback about what happened and what our next steps are in terms of the work in and of itself.
[52:00.20]  So, ironically, I think this is the opportunity for us to put two things onto the list that we actually want to do.
[52:09.20]  Does that make sense for folks?
[52:11.54]  Just give me a thumbs up.
[52:13.48]  Cool.
[52:14.44]  Good.
[52:14.66]  Okay.
[52:16.24]  So, all of that said, I did expect, because I had not had anybody queued up to present today,
[52:28.24]  that today's discussion would be...
[52:30.20]  It would have been shorter than it had been in the past.
[52:33.58]  This has been incredibly useful for us.
[52:38.44]  I definitely...
[52:39.80]  We will have another meeting in April, so you will see the action plan prior to it going to the conference itself.
[52:49.40]  And thereby, you will have ample opportunity to provide feedback on it.
[52:53.38]  But I do encourage that if people want to have a look at the overall write-up and get feedback on it,
[52:59.44]  I would very...
[53:00.20]  I would very, very much appreciate that.
[53:04.60]  And we will...
[53:06.16]  Yeah.
[53:07.44]  And so, we will look at both the action plan and the pre-materials that we have for the conference.
[53:17.18]  I may exchange some emails with folks in the next few months just to help us refine some of those materials.
[53:25.02]  But aside from that, I find this incredibly helpful.
[53:28.56]  So, I do apologize.
[53:30.20]  Or do request in advance that I may send out a random survey to folks to, like, a survey monkey or whatever type of thing
[53:41.22]  to ask for advice as we're building out some of the materials so that we're getting some advice from you folks along the way.
[53:49.54]  That would be very, very helpful.
[53:51.92]  But aside from that, I think I am at the end of what I wanted to talk about today,
[53:56.58]  unless there's anything that I have forgotten about.
[53:59.88]  No.
[54:00.20]  And any last remarks that people want to say?
[54:07.26]  Any famous last words?
[54:08.92]  Any famous last words?
[54:10.26]  I'm also empathetic because it's now 1.15 for Yvonne.
[54:16.78]  So, thank you, Yvonne, for staying up late.
[54:21.96]  And I'm hoping...
[54:24.76]  Well, thank...
[54:27.64]  Yeah.
[54:28.34]  Yeah.
[54:29.44]  I...
[54:30.20]  I am tripping over my words.
[54:31.78]  I'll simply say thank you for the fabulous feedback and engagement, as always, that I get with this group.
[54:38.96]  And I'm looking forward to when we meet in April, we'll re-say what opportunities and what shaping we're doing for the session in May when we meet in April.
[54:54.64]  Okay.
[54:55.24]  Cool.
[54:56.14]  So, with that, have a good rest of your days.
[54:58.70]  Be it, you know...
[55:00.00]  Early in the morning or late at night.
[55:04.18]  Thank you, everyone.
[55:05.20]  Thank you.
[55:05.68]  Bye.
[55:06.90]  Thank you.
[55:07.50]  Bye.
[55:07.70]  Thank you.
[55:07.94]  Bye-bye.
[55:08.12]  Thank you.
[55:08.56]  Bye-bye.
[55:09.42]  Thank you.
[55:10.26]  Bye.
[55:10.58]  Bye-bye.
[55:10.94]  Thank you.
[55:11.92]  Happy New Year.
[55:12.64]  Happy New Year.
[55:13.86]  Happy New Year.