[00:00.00]  then have uh as a recourse yeah no so i i think i think that is uh uh an excellent point so um
[00:11.84]  i would make a note to my colleagues who are here with me um to make sure that we do include
[00:17.28]  redress explicitly within the action plan um i think there's a logical place for it so we just
[00:22.08]  have to find it so that's good uh yes yes thank you very good to see you again um yeah i think
[00:29.44]  what ali said actually it will fit there in any case because when we say guardrails i guess it
[00:36.24]  will also involve regulatory stuff but also unavoidably at some point popping liability
[00:42.48]  and as you said we want liability to continue with liability so i guess that may be the place
[00:49.36]  compared to the three parts i see there that it would be also discussed and what ali said
[00:59.44]  and redress and so on i guess these are liability issues and there as we discussed eric some weeks
[01:07.68]  ago of course liability could get into manufacturer's liability into the user's
[01:12.88]  liability and then get a bit more interesting there but uh yeah i think uh regulation
[01:21.68]  liabilities are kind of two sides of the same coin somehow yeah and so uh yes
[01:29.44]  and uh i definitely think like liability obviously i would dare say it's one of the
[01:36.16]  key actions the action plan but we should make sure that we do associated with the redress point
[01:41.04]  that elise was bringing up before i'm going to go to anton and don oh sorry no i was thinking um
[01:58.24]  this is a great opportunity
[01:59.84]  to validate a lot of the questions and discussions we have been working in this
[02:04.32]  group but also most of the experiences that i have in this type of conferences
[02:10.64]  i think is key in um to prepare the the ministers of our countries or the or the
[02:19.60]  or the technical people that that are going to be there so just an idea is to um from this group
[02:28.24]  to create for
[02:29.84]  i mean for the countries we represent uh one pager with key information of what's really going on in
[02:37.92]  the country in terms of i don't know we can define in terms of the ai in terms of digital
[02:43.44]  transformation specific needs and specific questions so that they they can have this as a
[02:50.80]  tool during the conference to to get deeper and to get more from the conference so just an idea you
[02:57.60]  can consider it thank you
[03:00.40]  yeah so uh i like that idea um my the the intention of the the attendees the conference
[03:08.48]  is that they're not they're there as experts so they know the content but having said that i do
[03:17.28]  think them being equipped with this is what the lay of the land is locally would certainly help
[03:26.88]  um one of the things that we have been
[03:29.44]  were considering writing ahead of the conference as a sort of big pre-read for the participants
[03:37.90]  was a scan of the typical barriers that people talk about as reasons why we are unable to
[03:51.04]  responsibly scale AI solutions. Some of those barriers are real, some of them are fictitious,
[03:56.72]  but they're used as rationales and reasons. Because theoretically, each of the actions we
[04:04.60]  have in the action plan should be associated with one of the barriers. That's a theoretical way of
[04:11.04]  thinking of it. So that is one of the other things we were thinking of writing in the next
[04:16.56]  several months, just to make sure that we had a broad perspective. And that might actually be
[04:24.34]  a good thing for me to work on.
[04:26.72]  With this group to make sure that we have perspective on that. That's cool.
[04:31.64]  And we're happy to prepare because, again, it's a great opportunity. And the equity and the
[04:38.94]  challenges of the countries that are going to participate are different. So we have different
[04:43.44]  completely needs in Mexico and Costa Rica than in France and UK and the US. So I think it's a
[04:51.54]  great opportunity to develop what else is needed in terms of
[04:56.72]  the rest of the world. Thank you.
[04:58.40]  Thank you, Anton. Awesome. We'll go with Laura.
[05:02.88]  I want to underscore what Anton just said about the importance of preparation.
[05:09.36]  I think it would be good in terms of pre-work for the country delegates to come knowing
[05:14.24]  where sort of liability sits now. In our circumstance in the US, just as an example,
[05:21.60]  much of it sits with the provider that's using it. And it's still
[05:26.72]  right now sits on their shoulders. And there are some other aspects of where patients might go for
[05:33.70]  redress. But I think having them come with some landscape knowledge of their own situation would
[05:40.36]  be helpful. And many, of course, will come prepared. But I think it might be good if it
[05:43.82]  was systematically requested. I think another suggestion that I would have is oftentimes when
[05:52.48]  I'm participating in liability conversations, people want to identify themselves. And I think
[05:56.72]  it's important to identify the single entity or the group that should be held liable. And I think
[06:01.46]  we're going to have to for a period of time when this is, when we're really discovering and
[06:07.10]  understanding. No one really knows how this plays out. Just thinking about entertaining the idea of
[06:15.10]  shared liability for maybe a period of time. So just that concept. I would also suggest that if
[06:24.62]  we're not already in a situation where we're not already in a situation where we're not already in a situation
[06:26.70]  where we're not already in a situation where we're not already in a situation where we're not already in a situation
[06:26.84]  where we're not already in a situation where we're not already in a situation where we're not already in a situation
[06:26.90]  including AI as a participant in this meeting, that asking of the, of AI, large language models,
[06:35.18]  and maybe several of them, Claude, you know, whatever, to respond in the same way to maybe
[06:41.86]  help us with generation of truly creative. So we don't come with our own sort of country views or
[06:48.56]  our own locked in sense of where liability should set, but ask large language models to be truly
[06:54.84]  thoughtful, creative, and come up with some sort of solution. And I think that's a very, very good
[06:56.68]  That is truly novel. The last thing I'll just say is I don't know to what extent, since this is focused on what governments should do. We just had a recent announcement last week of the relaxation, so to speak, of it allows those that are developing direct-to-consumer tools to quite significantly skirt our FDA in the U.S.
[07:25.08]  and not necessarily have to go through that process in order to bring things directly to, and this could include things that might be considered and formerly should have been going through the regulatory process, blood pressure,
[07:40.84]  these types of things that are then going to be incorporated into our clinical records here, but they will not have come through any formal regulatory.
[07:49.06]  So I think there is that burgeoning issue of things that are sort of allowed.
[07:55.08]  I don't know to what extent you would be considering that, but it just seems, I know it's going to be material here as we start to watch things like the chatbots that have resulted in some teenage suicides here.
[08:11.20]  Those are direct-to-consumers and heretofore not regulated.
[08:17.02]  And not just there. The teenage suicides are happening in other parts of the world as well.
[08:23.54]  I think that's where it's...
[08:25.08]  It is good advice for me to broaden what language I'm using in this space, that it's not just direction to governments, it's direction.
[08:35.52]  I think that there needs to be some language that's reflective of the actions that are taken collaboratively across public sectors, private sectors, and the public.
[08:48.22]  Exactly.
[08:48.64]  And collectively, everyone is working together to work on a name, and all three entities are accountable for taking action.
[08:55.08]  And being involved in and disseminating, et cetera, the action.
[08:59.84]  So I need to be smarter about my work, careful about my language, because I think that's an excellent point.
[09:07.58]  Cool. Olga?
[09:10.56]  Thank you so much, Eric.
[09:12.32]  Actually, my point echoes or continues the theme that Laura has brought up about direct-to-consumer devices, but also the emergence of Doc2Me.
[09:23.42]  You know, for many years, it was...
[09:25.08]  It was Dr. Google that people went to, but now it has gone further than that, and people actually interact with AI in a very sophisticated way, especially recently we've heard about both Anthropic and Chet GPT acquiring specific healthcare capability through new acquisitions.
[09:41.66]  So there'll be more really high-level, sophisticated interactions between consumers and AI on that, and I think the notion of accountability and liability...
[09:55.08]  And kind of safeguards in this area is also very important.
[10:00.50]  And just to express very strong support for the clarity around medical, legal liability for healthcare professionals.
[10:11.66]  I've mentioned before the Nuffield Trust report, how GPs use AI in the UK that our company supported.
[10:19.72]  And it's absolutely fascinating that the main barrier for adoption of...
[10:25.08]  Of AI tools, both for doctors who use and those who do not use AI, is the fact that they don't really know who's responsible for any...
[10:33.70]  If anything goes wrong, because there is lack of clarity for this in the UK, and I suspect in other places as well.
[10:40.10]  So overcoming this would absolutely be something that would facilitate acceleration of AI in healthcare.
[10:49.18]  Thank you.
[10:50.46]  Great.
[10:51.00]  Great.
[10:52.04]  I wonder if that's a general...
[10:55.08]  Lens we should be prepared to take on the overall session is the whole Dr. Me, so to speak.
[11:08.08]  Because literally in the last two, three weeks, I keep on getting bombarded with, oh, look, more or less all the big tech guys have done their health-specific version of their LLM.
[11:20.42]  And so understanding that that is where the world is now, and how do we respond to that effectively, I think is one of the important things to think about.
[11:35.34]  Yeah.
[11:36.08]  Cool.
[11:37.72]  Yvonne?
[11:40.26]  Happy New Year, Eric.
[11:41.92]  Happy New Year, everybody.
[11:44.40]  Yes.
[11:44.92]  So I'd like to start by saying, yes, I agree with...
[11:49.42]  I agree with what Elise has said about needing to talk about recourse, and also with Laura about shared liability, and the word shared is the operative word here.
[12:04.18]  I think, and Eric, correct me if I'm wrong, but as the only physician here in this call, while we're talking here, I think it's really important to advocate for humanity.
[12:19.42]  Yes.
[12:21.22]  AI is not coming to this conference.
[12:23.88]  It does not actually have a seat on the table.
[12:27.36]  It is humans here, and we are as humans.
[12:31.08]  It's a human conference.
[12:33.12]  So I think we need to ensure that we are doing the absolute best for the humans.
[12:39.28]  And I'll elaborate a little bit more as I keep going.
[12:43.50]  So far, we talked about things related to liability, which is kind of like, in a way, if you like, it's kind of like the negative.
[12:49.22]  It's a negative phase of what we're talking about when things go wrong.
[12:53.16]  And yes, I totally agree that things will go wrong.
[12:57.20]  A few things before I get into what we could also do as a positive thing for the conference is I think it needs to be a little bit more in addition to guardrails, a little bit more in addition to humans in the loop.
[13:12.54]  Because, for example, humans can be in the loop.
[13:16.56]  But the human may not have all the...
[13:19.22]  Perspective, because that human who is in the loop may not have been the person who invented, who developed the AI.
[13:30.76]  So what am I talking about being positive?
[13:32.94]  So in addition to recourse, in addition to shared liability, I think it's really important to have in the conference to ensure that we talk about the four pillars of ethics.
[13:48.62]  So the things...
[13:49.22]  To highlight beneficence, non-maleficence, in other words, do no harm.
[13:56.34]  So all the AI that come into fruition need to pass this test.
[14:02.84]  Beneficence, non-maleficence, autonomy, which means that if the patient should choose not to have AI in their care, they should have the right to do it and have equal care and justice, which means that it doesn't really matter who you are.
[14:19.22]  There's equitability, because we need that for the world.
[14:25.06]  And I think it's really, really important that this becomes a topic in addition to recourse, which I think is really important, and in addition to shared liability.
[14:38.68]  Thank you.
[14:40.68]  One, doubling back, Yvonne.
[14:43.74]  You said four parts, and I wrote down three.
[14:48.42]  Oh.
[14:49.22]  So beneficence, non-maleficence, autonomy, and Rachel's specific justice.
[14:55.16]  Okay.
[14:56.22]  Cool.
[14:58.00]  One of the other things I was thinking of, obviously, I have now moved on to talking about the action plan itself, but I'll reflect it directly in a minute.
[15:10.28]  One of the other things that I was thinking of doing in parallel in the next few months is a small...
[15:19.22]  One of the other things that I was thinking of doing in parallel in the next few months is a small write-up that, because intentionally I'm trying to get the two pages to be easily consumable, as easily consumable as I can, but at the same time, to make it consumable, you lose some of the nuance.
[15:34.80]  And so what I was thinking was to have a slightly longer write-up around each of the specific sub-actions that are on the second page, and that's where I would bring in...
[15:49.22]  I think that's exactly the point that you were just talking about, Yvonne.
[15:53.52]  So I think where I was thinking about it in the back of my head, I'm now thinking it needs to come to the front of my head in terms of something that's important to do.
[16:03.84]  So that was me just consuming, thinking about that.
[16:06.64]  Oh, sorry, Eric.
[16:08.78]  So I forgot one more thing that I wanted to add.
[16:10.92]  Oh, yeah.
[16:11.08]  We've always talked for the whole, you know, for the year and so on, all the time we've been here about improving trust.
[16:19.22]  And having this, the bioethics part in this, would improve trust, because we, you know, we have nothing against AI.
[16:29.66]  We just want to make sure that it is good, but we have everything against bad AI, and all of us here also don't want bad AI.
[16:38.40]  So all this, the bioethics part will also contribute towards trust and moving everything forward, as you have outlined to us, Tara.
[16:49.22]  Thank you.
[16:51.32]  So with that said, I will move on to reflecting the action plan.
[16:58.50]  And directly, I'll reflect your comment, Yvonne, because in the action plan, as I redrafted it, and thank you all for the feedback that I received on it between December and January, one of the important things that I was able to do in the action plan was to reflect the action plan.
[17:17.62]  And I was able to do the action plan.
[17:19.22]  And one of the things that I was able to do in the action plan was I moved this section on earned trust from the, more or less, the rightmost section to being the leftmost in the action plan itself, being at the bottom part of the page to the top half of the page, to the point now where the first line of the action plan, the detailed action plan, is embed ethics and respect for human rights and require the AI system use to demonstrate positive benefit for patients and to publish.
[17:48.40]  So, yeah.
[17:49.22]  Very much trying to get to that point with us.
[17:53.32]  So all of that said, Yvonne, strong agreement.
[17:58.10]  And bluntly, Yvonne knows that was her specific suggestion.
[18:03.80]  So, yes, I embrace that suggestion in terms of the action plan itself because it made a lot of sense to me.
[18:10.86]  Because a lot of part of what we've talked about within this forum in the span of the last year and a half has been,
[18:18.16]  how do we reflect that doing AI and health necessarily involves a lens of ethics?
[18:27.56]  So, hence why putting it up front, making it clear, I think is one of the strong things we can do.
[18:35.26]  Okay.
[18:36.48]  So, let me segue for a bit.
[18:41.16]  So, let me talk a little bit about the action plan itself.
[18:44.88]  Some of you, so Sarah attached the updated version of the action plan earlier this week.
[18:54.20]  So, it's on the invitation for this meeting.
[18:56.92]  I'm not going to go through it in depth right now, but I wanted to touch on some of the aspects of it.
[19:08.50]  I do invite that if people do want to provide additional feedback on the action plan,
[19:15.36]  I'm still very happy to receive that in the next few months.
[19:20.26]  And because of how this conversation has gone, it's actually changed some of my idea around what some of our next steps are.
[19:29.50]  Cool.
[19:31.90]  Well, I'll just take Leah's suggestion as being good and trust that somebody is recording.
[19:39.64]  So, cool.
[19:43.24]  So, the action plan.
[19:45.08]  So, the action plan itself, the, you, back in our September meeting, we actually did have an earlier version of the first page of this plan.
[19:57.56]  I had started to develop this with some colleagues in March of last year.
[20:06.48]  And that's why it's also great to have received such good robust feedback.
[20:11.72]  All the stuff that you see in the action plan in red.
[20:14.88]  It's just simply reflections of where I've changed or added words because of the review of this group.
[20:24.52]  And as you can see by the sheer amount of red on the document, there was a lot of very good positive feedback.
[20:31.06]  The general feedback was the, a lot of the, the action plan itself covered a lot of the ground that needed to be covered.
[20:40.24]  But some of the words or focus areas within the action plan itself.
[20:44.60]  Could have been, could be strengthened.
[20:46.46]  And that's why I think that is, or at least I hope that is what we have managed to achieve with this over the last little while.
[20:55.34]  Also emphasizing the importance of partnership across those three entities, the public sector, the private sector, and the public toward a common aim.
[21:05.20]  And importantly, and I think I might even strengthen reflective of our earlier conversation.
[21:11.50]  But why cooperative action matters now is a longer write up.
[21:20.86]  I would certainly reflect that.
[21:22.96]  Listen, ever, there are dozens of these LLM health specific things that are being put out there.
[21:30.10]  And people are using them.
[21:31.28]  There was a study back in last month that what was it?
[21:36.88]  One.
[21:37.88]  I don't know.
[21:38.88]  I don't know.
[21:39.88]  I don't know.
[21:40.88]  I don't know.
[21:41.04]  I don't know.
[21:41.14]  I don't know.
[21:41.16]  I don't know.
[21:41.22]  I don't know.
[21:41.28]  I don't know.
[21:41.50]  I don't know.
[21:41.90]  I don't know.
[21:41.94]  I don't know.
[21:41.98]  I don't know.
[21:42.00]  I don't know.
[21:42.02]  I don't know.
[21:42.04]  I don't know.
[21:42.06]  I don't know.
[21:42.08]  I don't know.
[21:42.14]  I don't know.
[21:42.16]  40% of people, or no, one third of people believe that they can be, understand their own health as good as a doctor's advice can give them by using the internet after one week.
[21:58.12]  So one week and I'm as good as a doctor.
[22:01.66]  That was, yeah, I certainly agree with this.
[22:05.38]  Oh, no.
[22:06.60]  But sadly, now in the same survey.
[22:09.24]  Ironically, or maybe happily, 40% of people reported regretting making a decision and taking action based on advice they gleaned from the internet without advice from a doctor.
[22:27.06]  So good news.
[22:29.22]  People actually do respect and value the advice they get from the medical system, from the health system itself.
[22:36.28]  Bad news.
[22:37.34]  They still think that they can actually do it themselves.
[22:41.72]  So, and they have been led to believe that they could probably achieve good outcomes themselves.
[22:48.64]  And I don't know if folks heard, some of the work of AI has started to become semi-autonomous, where in Utah, there was approval for AI to provide prescription renewals for people without intervention.
[23:07.24]  Or a medical provider.
[23:10.14]  That's, yeah.
[23:12.50]  Yeah.
[23:12.76]  So sorry.
[23:13.42]  I'm just seeing the expression in your face.
[23:16.74]  I'm sorry.
[23:18.38]  It's like, yeah, sadly yeah.
[23:22.66]  So, I mean, things, things, things are happening and happening fast.
[23:27.40]  Like I don't know where we thought the world would be one year ago.
[23:32.16]  But if we project forward to where the world is going to be a year from now.
[23:36.86]  It's not going to be a linear exercise between where it was a year ago into today, it will
[23:42.24]  be an exponential exercise.
[23:44.68]  So if we think things have gotten better by two, they will get better by another factor
[23:49.64]  of four from here, or different by a factor of four, and that's really better, which is
[23:55.56]  exactly the point that we're having.
[23:58.78]  So the point of the action plan, as you can see, at the high level for the first page,
[24:10.62]  it really reflects on the need to grounding in an earning of trust, which reflects what
[24:18.48]  I've heard from this group time and time again, and what Yvonne said earlier on in her intervention.
[24:25.40]  Then we want to make sure that we do enable what works.
[24:28.34]  But as we do enable what works, we want to make sure that we do enable what works.
[24:28.76]  As we are enabling what works, we need to make sure that we are preventing what works.
[24:33.42]  And so within that, the lens of embedding bioethics and respect for human rights, making
[24:41.78]  sure things are transparent, explainable, not sure if we need to expand that area a
[24:47.08]  little bit, and I'm not sure if we have engaging patient providers in the public, that might
[24:56.30]  be something that we want to strengthen.
[24:58.32]  As opposed to just engage, but actually be tempted to change that to involve.
[25:07.54]  Because engage, you can do that, but that actually taking into account the considerations.
[25:15.04]  And then the last part about expanding education, just to make sure that people understand what
[25:19.66]  the heck is going on.
[25:21.66]  So that's certainly one thing I would think of from the earning trust bucket.
[25:28.32]  I'm going to pause for a sec.
[25:30.82]  Let me summarize the earning trust bucket and see if there's feedback on it.
[25:41.48]  So the earning trust really is, if I can paraphrase what's there, put human first, respect ethics,
[25:52.12]  make sure that you're doing things transparently and openly so people understand.
[25:56.36]  I'm going to pause for a sec.
[25:57.36]  Let me summarize the earning trust bucket and see if there's feedback on it.
[25:57.88]  So that's certainly one thing I would think of from the earning trust bucket and see if there's feedback on it.
[25:58.38]  I'm going to pause for a sec.
[25:58.90]  Let me summarize the earning trust bucket and see if there's feedback on it.
[25:59.40]  We involve all the relevant stakeholders in the overall work.
[26:04.78]  And we make sure people understand what is happening in their system, that they have
[26:09.44]  the tools and the knowledge in order to properly engage with this new AI system right from
[26:15.84]  the public, right through providers and the support staff, etc.
[26:19.84]  And there should probably be something about capacity-building in there, too.
[26:27.42]  So when I think of trust, if we have that, the ethics, the human aspects, being transparent, involving the public and making sure people have the skills and knowledge in order to do this work.
[26:39.54]  Is there anything massively missing in that bucket?
[26:43.08]  I see Leah.
[26:47.08]  My hand is not for anything that's massively missing.
[26:50.50]  I put it up before you asked the question.
[26:52.72]  I'm just wondering if there might be a couple of points here where there would be benefit of cross-border collaboration.
[27:04.30]  It might be helpful if you kind of level set what's the bar by which you mean collaboration.
[27:10.80]  Because I think you said earlier, it's not necessarily sort of like learning from each other.
[27:14.92]  It sounds like it might be a more intensive cross-border collaboration.
[27:19.70]  But anyhow, for some of these points.
[27:22.72]  Like number two, that might be an area where cross-border collaboration might benefit.
[27:29.74]  I know there's a lot of struggle around thinking about things like consent and what does that look like, how do you do it, whatever.
[27:34.34]  But yeah, so that was my one piece.
[27:38.46]  And particularly number two.
[27:39.62]  But once I have a better sense of sort of where the bar is for what's meant by collaboration, it would help me just think through.
[27:45.96]  Are there certain points here where multi-jurisdictional collaboration might be a benefit?
[27:52.72]  When I think of that bar, I'll just say my thoughts and others can weigh in, too.
[27:58.80]  And these are areas that by setting some consistent guideline, it means that the implementation and development of AI solutions is faster, more trusted, more effective.
[28:19.42]  So explicitly out of that first bucket of four.
[28:22.72]  I would explicitly highlight that same one that you just highlighted.
[28:26.80]  Because I think that is something that there is benefit in multiple entities agreeing what is sufficiency for transparency and explainability.
[28:35.68]  Because that sends a clear signal to the development community about what they have to do in order to make things explainable.
[28:46.50]  Whereas I can educate however I want.
[28:48.90]  I can learn from other countries, but it doesn't need to be done the same.
[28:52.72]  The content would naturally be similar.
[28:56.18]  But how it is done, yeah, I don't think countries need to be doing it the same.
[29:02.20]  Whereas guidance around transparency and explainability, that helps reduce cost burden by being similar or compatible.
[29:13.36]  Great.
[29:13.70]  Yeah, thank you very much.
[29:14.96]  Okay.
[29:15.26]  Thank you, Lynn.
[29:17.04]  Come on.
[29:20.66]  Thank you for this.
[29:21.90]  I was just going to say.
[29:22.72]  Eric and team.
[29:25.02]  It must have been very hard to try to squash it all in, right?
[29:29.06]  You've done a really good job.
[29:32.12]  Thank you.
[29:32.82]  Thank you.
[29:33.66]  Seriously.
[29:34.28]  I mean, obviously, you know, I sent stuff back to you all.
[29:37.68]  Mine wasn't very good, but we all agree on the points.
[29:41.34]  What I wanted to say was that on, you know, the first one that says embed ethics, the action plan, I wonder, and I'm happy to kind of like,
[29:52.76]  be guided by you all as well, whether it should be bioethics or somehow, so it's human ethics, it needs to be very clear.
[30:02.74]  It's ethics for the humans.
[30:06.08]  So generally, we use the word by ethics rather than just ethics.
[30:10.34]  Because, you know, it can be reinterpreted a different way if you just put the word ethics.
[30:17.78]  It could be ethics towards AI.
[30:20.66]  And we.
[30:21.34]  We.
[30:22.06]  You know.
[30:22.56]  Yeah.
[30:22.66]  We want to put humans first, which is what we are talking about here.
[30:26.04]  So just make sure, you know, my suggestion would be bioethics,
[30:30.10]  but feel free to kind of like, you know, think about something else,
[30:33.68]  but it can't just be ethics.
[30:35.08]  That's one thing.
[30:35.90]  The other thing is about the words beneficence and non-maleficence
[30:43.16]  because they mean something specific.
[30:47.56]  I don't know how you're going to put it in,
[30:49.10]  but I think it's really important.
[30:50.88]  And the very, very last thing is I can see you've got the demonstrate
[30:55.42]  positive benefits for patients, and I'm also guided by you all
[31:00.52]  as to whether there's got – because people will argue,
[31:05.66]  well, there is positive benefit, and there could be like a thousand
[31:10.24]  other negative benefits, but they go, well, you know,
[31:13.38]  I have shown positive benefit.
[31:14.94]  So I wonder whether it could be something like,
[31:20.02]  you know,
[31:20.88]  it's a – there are somehow – somehow do this,
[31:26.60]  but more positive benefits than negative outcomes.
[31:32.88]  So you have to have a positive – what we call positive benefit risk balance.
[31:39.84]  This is what we say in the Australian system.
[31:44.22]  But it has to be more positive, not just positive.
[31:49.00]  It has to be more positive than negative.
[31:50.88]  So what we use is positive risk balance.
[32:00.64]  Oh, I like that.
[32:01.48]  I like that very much.
[32:02.88]  Thank you.
[32:03.64]  And you already – your point around bioethics earlier on.
[32:08.76]  So it's in there right now, and we'll see how it goes.
[32:12.00]  But for now, I've changed it to bioethics.
[32:14.76]  But I actually very much like your statement because, well, it's positive over here.
[32:20.24]  Ignore all that stuff.
[32:22.06]  You know, like that – when you think about climate change,
[32:25.02]  hey, we've had great positive benefits.
[32:26.72]  We killed the planet, but great benefits.
[32:28.50]  So hence why bringing in the yin and yang, that risk balance aspect,
[32:34.16]  I think is really quite practical.
[32:36.70]  And because it's what we're trying to do.
[32:39.34]  Cool.
[32:43.58]  So in turn – let me quickly go through.
[32:47.24]  No, I'm good.
[32:48.38]  The second –
[32:50.24]  The second bucket, so in this, where we talk about enabling what works,
[32:56.66]  focus here is around strengthening our data infrastructure,
[33:02.34]  making sure that we have good ways from a lifecycle perspective of evaluating
[33:08.14]  and assuring that AI solutions are effective.
[33:12.00]  Once they have been approved, that we can effectively and accelerate the integration
[33:16.52]  of those solutions into care practices and workflows.
[33:19.76]  And scale those effectively.
[33:22.22]  And then we want to make sure all of that within an environment where our workforce
[33:26.80]  has the capacity and capability to utilize these tools that are being created.
[33:34.24]  So that was our focuses around enabling what works,
[33:38.04]  which in a way is reflective of the overall lifecycle.
[33:41.82]  Make sure your data foundations are strong.
[33:44.54]  Make sure that you can improve the systems, develop and improve the systems.
[33:48.36]  Make sure you can scale them.
[33:49.76]  Monitor the systems.
[33:51.04]  Make sure that we can use the systems effectively.
[33:55.64]  It's always fun doing this and summarizing it in as few words as I possibly can.
[34:01.20]  So that was the ethos around the enable what works.
[34:04.84]  Does that sound reasonable?
[34:08.20]  What did you just say, Anton?
[34:10.84]  Yeah.
[34:12.10]  Yeah.
[34:15.30]  Yeah.
[34:16.44]  Yeah, that's very true.
[34:18.24]  All right.
[34:19.76]  So this is where using AI, bringing a lot of technology to rural and remote communities,
[34:28.76]  while it has risks associated with it, then is it more risky to have an AI solution there
[34:38.10]  than to have nothing there?
[34:40.72]  And if you asked me three years ago, the answer would be yeah, clearly.
[34:44.88]  Whereas as in it's more risky to use AI than nothing.
[34:47.98]  But now the question is,
[34:49.76]  I think there is a risk balance, if I think about Yvonne's words from a few minutes ago,
[34:55.94]  to really think about that.
[34:57.90]  So that I think it really, I wonder if there's an exercise in all this that really reflects
[35:04.40]  on what some of the risk balance areas are.
[35:07.26]  That's something for us to think about, especially what some of the key use cases are
[35:11.52]  that we should be prioritizing.
[35:13.62]  So there's the action plan we should prioritize.
[35:16.38]  Ah, that's something else we could prioritize.
[35:18.38]  Is the actual use cases that could be where AI can be deployed,
[35:26.32]  where we should focus our efforts.
[35:28.30]  That's cool.
[35:29.20]  Janice.
[35:31.28]  Yes.
[35:32.88]  Actually following a bit what Antoine, sorry, says there.
[35:37.56]  Yeah, indeed, that's another interesting issue there.
[35:40.38]  I think which could also get into enabling how maybe the,
[35:45.50]  two things there, I think.
[35:46.86]  One of them is,
[35:48.38]  how an AI system is also performing in diverse environments.
[35:55.98]  So maybe in rural areas or in different environments,
[36:00.22]  because it might be FDA or CE marked approved,
[36:04.28]  but maybe for different types of reasons,
[36:08.14]  because what FDA or CE marking provides in Europe,
[36:12.38]  it would be that it's performing technically okay,
[36:15.50]  under those conditions, parameters and so on.
[36:18.32]  But it could be the case that it performs differently
[36:26.72]  in different environments, depending on number of factors.
[36:30.16]  And I think also in rural areas or different areas,
[36:33.40]  different types of people and so on,
[36:36.00]  not only on data of people,
[36:37.64]  but also even clinical workflows, for example,
[36:41.02]  could be affecting a bit the education of the physician,
[36:44.38]  the interaction, for example,
[36:46.38]  of how physicians interact with AI,
[36:48.26]  how they're assessing the AI system,
[36:51.96]  how they're applying it and so on,
[36:54.42]  compared let's say to a university hospital
[36:56.70]  where a professor is using it and so on.
[36:58.94]  So I think all these indeed are factors there.
[37:03.16]  And secondly, indeed also when it comes to rural areas
[37:10.16]  and so on, maybe that's something we should think
[37:13.02]  of the value of AI in remote areas,
[37:16.50]  because that will also give us,
[37:18.26]  I think the idea of equitable,
[37:22.02]  if I may call it under the heading of equitable enjoyment
[37:26.42]  of AI by society at large.
[37:30.14]  So we don't create another digital divide between,
[37:34.96]  now as we had in the past, for example,
[37:37.06]  digital divide on other stuff.
[37:39.76]  Now we don't create another new AI divide
[37:42.68]  between those who can enjoy AI in metropolitan cities,
[37:47.32]  for example, in big cities, for example.
[37:48.16]  We can have big university hospitals and so on,
[37:50.46]  and we relieve other people in rural areas
[37:56.22]  or in other countries behind from these enjoyment
[37:59.88]  and revolution.
[38:01.64]  So I think, yeah, these are kind of interesting aspects,
[38:05.24]  I think that we could also see.
[38:08.94]  Thank you.
[38:10.02]  Yeah, thank you, Ernest.
[38:11.88]  I'm thinking through how to incorporate that
[38:16.88]  incorporate that into what we're thinking uh in terms of the agenda I think that maybe there's
[38:23.36]  even um maybe there's actually a special section that is thinking about given all the actions that
[38:28.94]  we've talked about here what are the use cases where there is benefit and which actions benefit
[38:35.54]  which use cases I wonder if there's something crazy like that um so I I let me do some thinking
[38:45.14]  about that um and for what it's worth Janos I have uh been using I had started using a line
[38:52.28]  uh a year ago about we're at risk of um digital divides expanding into digital chasms
[39:01.94]  um because of AI so we want to make sure that if we don't uh and the additional chasm is
[39:09.56]  untraversable so digital divide arguably you can still get a bridge
[39:15.14]  um it's a lot harder uh and that is the the path that we're we're we are at risk of going down
[39:23.36]  cool uh Laura Eric I think you're on to something with this use case
[39:29.84]  um this feels it's resonating with me because we have thought about that here too at the National
[39:36.26]  Academy and so one of the uh initiatives we're launching on March 3rd is to build the first ever
[39:43.76]  National strategy
[39:45.14]  for patient safety AI enabled patient safety and this is not to um not to be another effort to make
[39:52.52]  AI safe it's an effort to deploy AI to make care safe and but de facto we'll be using it for
[40:00.32]  measurement of harm and that would be harm from all sources including AI so it's almost a Trojan
[40:06.74]  horse if you will a way to get more of the attention on making AI safe without explicitly
[40:15.14]  looking directly at that which which some perceive as slowing down progress so our first ever National
[40:22.16]  strategy will be patient safety at scale meaning what parsimonious four to five uh actions four to
[40:30.50]  five areas where we take one to two actions it's not going to be a 40 point strategy with 25 bullets
[40:37.10]  under each it's going to be the highest leverage actions we can take to enable every delivery system
[40:44.36]  in the U.S.
[40:45.14]  to perform better on safety using AI as the as a means to doing that and the reception that we're
[40:53.06]  getting um people have said this is this is the way to break through this is an area where it's
[40:59.42]  unassailable nobody stands up and says we don't care about safety in fact people you you would
[41:06.50]  never stand up and say this is not important uh it's it's primacy really do no harm um is the first
[41:15.14]  step uh and if we can't make that promise then I don't know about the rest of them so I I just
[41:20.66]  support this idea that you're thinking about this is not necessarily I'm saying use the safety use
[41:25.34]  case but I think this idea of bringing things through in a single more bounded use case makes
[41:32.72]  it more doable and especially choosing a use case that um is defensible uh to the Max yeah
[41:41.30]  um and for what it's worth I like the safety use case
[41:45.14]  um so uh thank you um by there there are two angles I would bring into that Laura uh one is
[41:54.20]  uh been working with a researcher out of Alberta around data related homes and reflecting that
[42:01.40]  sometimes there are harms that come from the non-use of data as much as there are arms that
[42:07.10]  come from the use of data so we actually have to balance it back and forth the other uh report
[42:12.86]  that we actually came up with
[42:15.14]  uh as the OECD back in March last year um was around diagnostic safety um and it was in that
[42:23.00]  report that reflected that uh 15 of health care encounters have a diagnostic error which which
[42:31.40]  contributes to 17 and a half percent of health system costs and so that in a way that very much
[42:39.92]  ties to the safety train so it's something that we as an organization actually have some
[42:45.14]  to say about um but I think it also as you said uh resonates with whomever you're talking to very
[42:52.34]  very very easily exactly it's like a diamond and you can shine many different facets of that Diamond
[42:58.58]  you can point it toward the sun so if it's someone who's concerned about uh waste in the system and
[43:04.70]  you know expenditures that don't have to especially if you're looking at total cost of care so they
[43:09.80]  may not feel that moral pull that others feel um they're looking at
[43:15.14]  economic fine um if that's the door that they want to enter uh in this then but I think it's
[43:20.84]  it's really an opportunity to to do a lot with AI and get it scaling in the name of something
[43:28.28]  um absolutely um essential to do yeah and it's no longer in the pursuit of a shiny object it's
[43:34.94]  actually in the pursuit of something we can all agree makes sense so good okay uh definitely have
[43:42.32]  more well and my brain is
[43:45.14]  going as it often does during these calls um thank you um
[43:52.10]  I think actually it's very very helpful uh so let me just go through the uh last column
[44:00.68]  um just to call them out so in the context of preventing harm um
[44:06.86]  we reflect the the need to protect uh privacy and security while preserving human oversight making
[44:15.14]  sure that we have safeguards after the solutions have been approved so the enabled bucket more or
[44:20.96]  less approves the AI systems and put them onto the market the prevent harm bucket makes sure
[44:26.66]  that we are monitoring the systems that both before and after it is implemented that there
[44:33.92]  are safeguards in place to make sure what is on the market is is safe and continues to be safe
[44:40.70]  um and then the last thing is that there are methods
[44:45.14]  to learn from each other uh as we're doing this work because like this is um the harm that is
[44:54.74]  experienced in one part of the system is an opportunity to prevent harm in another part
[44:59.62]  of the system so we want to make sure there are those channels for continuous learning across the
[45:04.98]  overall stuff uh overall ecosystem and so i'm very much leaning based what we've been talking about
[45:13.94]  if i really go to this area and think about diagnostic safety or not diagnostic that would
[45:20.50]  be too narrow uh because coming back out that there's some administrative work that is happening
[45:27.46]  um is but how do we make sure that these are the use cases where safety is paramount at all cases
[45:37.22]  yeah okay yeah there you go cool okay
[45:44.90]  uh well i definitely think that that has been as much as it's been a fast uh action through because
[45:53.62]  i think it's because what i'm hearing from folks apart from the words that are on the page what
[45:59.86]  would really help um is ironically and correct me if i'm interpreting this wrong um but what i'm
[46:10.66]  hearing folks say is it would benefit this is that it would benefit people who are doing this work and
[46:13.86]  it would benefit people who are doing this work and it would benefit people who are doing this work and
[46:13.92]  if there were a different page two which really reflected here is the key use case
[46:24.40]  which would be related to safety and in order for that use case to happen
[46:30.64]  these are the actions that we need to to see play out so really focus on it then we annex
[46:39.04]  that and here is a list of all the actions that were identified through the work that we did with
[46:42.80]  experts so that's the key use case which would be related to safety and in order for that use case to happen
[46:43.84]  so but we really narrow in the focus instead of having 36 actions
[46:49.84]  we'll have the i'll call it 10 who knows uh that would say these are the actions that
[46:56.72]  are most important to be taken in order to assure safety is that what i'm hearing
[47:04.80]  well it's what i'm hearing is that what i should have heard
[47:14.32]  for me when i was talking about safety um i i didn't mean again focusing directly on the
[47:20.16]  safety of ai but using ai to make care safe which puts everyone in the obviously there's an aspect
[47:29.04]  of that that says and the ai can't harm patients either but it comes in uh door with these other
[47:37.60]  things that are uh so it isn't perceived as much as people trying to slow down progress
[47:43.84]  so we're really coming at it from this idea of of the actual making care safe
[47:50.40]  using ai using it as the positive part but it can be yeah that that which is
[47:57.84]  different than what i've been talking about but it's still very related to what i've been talking
[48:03.92]  okay so let me get my head around this um i am i'm not exactly sure where we go
[48:12.48]  um
[48:13.84]  all the help is focused on the action yeah absolutely um yeah so i i like the idea
[48:22.08]  of i'm starting to enjoy the idea of having a second page in here that really says here is a
[48:28.96]  use case these are the actions that need to happen for that use case to be live here is another use
[48:35.60]  case these days so we have a couple of sample use cases that uh people can chew on and then we have
[48:43.84]  a complete list of all the actions and because then uh
[48:50.88]  uh i can see a table that is me being too geeky here are the use cases here are the actions and
[48:58.72]  which actions benefit most from which use cases sorry which use cases benefit most from which
[49:03.60]  actions then you can say here are the highest value actions to take
[49:07.60]  because they actually by doing this one action it benefits all use cases or optimal set of use cases
[49:13.84]  that is something that is missing today cool sorry uh following the conference can we develop an
[49:21.36]  agent tailors for government use well i don't think we would um
[49:30.80]  oh asia will also offer best practices case studies north sea countries well maybe we would
[49:36.16]  um so thank you for that anton that's interesting i'm going to talk to my my uh colleagues over in sti
[49:44.32]  so they're worrying about uh the the overall lens of um
[49:52.64]  of ai so uh and policy associated with it they actually created a policy repository over the
[49:59.44]  span of the last six years uh and so it would be interesting to talk to them to say uh because what
[50:06.48]  you suggested is a way they could operationalize that inventory itself yeah that's really interesting
[50:14.16]  okay um so this is the part where uh i changed what my mind was up front um because you know
[50:27.44]  i've been i've been threatening for or a couple of times that you know this group is going to wind
[50:34.24]  down at some point in time just have to invent that this group is going to wind down i had expected
[50:41.12]  that this would be among the last meetings that we would have but based on this discussion today
[50:48.10]  i'm not thinking it probably makes sense to do one more kick at the can again
[50:51.46]  see sarah's laughing at me because keep on doing this every time um but specifically i think that
[50:58.40]  some of the um i think having there are two there are two meetings that i think make sense to do
[51:05.50]  one which is one prior to the conference that looks at what materials we have we're developing
[51:11.70]  for the conference to make sure that we aren't having accidental blind spots around that
[51:17.18]  especially around the uh our thinking around the actions the evolution of the action plan as it's
[51:25.46]  gone through several iterations the next few months i think that would be incredibly beneficial
[51:30.34]  for us uh that i would suggest it probably might be in mid-april
[51:35.50]  um i would then suggest that we have um we would set up another meeting with this group
[51:42.22]  in a day mid-june shortly after the conference for those who aren't able to otherwise attend
[51:51.20]  the conference because of the constraints i've talked about before um to get feedback about what
[51:56.22]  happened and what our next steps are in terms of the work in and of itself so ironically i think
[52:02.22]  this is the opportunity for us to
[52:04.74]  um
[52:05.50]  put two things onto the the list that we actually want to do does that make sense for folks just
[52:12.02]  give me a thumbs up cool good uh okay so all of that said um i did expect um because i had not had
[52:24.44]  anybody queued up to present today uh that today's discussion would be a bit shorter than it had been
[52:31.70]  in the past um this has been incredibly useful to me and i think it's going to be a great opportunity
[52:35.50]  for us um i definitely i we will have another meeting in april so you will see the action plan
[52:43.86]  prior to it going to uh the the conference itself uh and thereby you will have an ample opportunity
[52:52.02]  to write feedback on it but i do encourage that if people want to have a look at the overall write
[52:57.78]  up and give feedback on it i would very very much appreciate that um and we will see you next time
[53:01.42]  thank you
[53:05.50]  yeah and so we will look at both the uh the action plan and the pre-materials that we have for the
[53:16.22]  conference i may um exchange some emails with folks uh in the next few months just to help us
[53:23.18]  refine out some of those materials but aside from that i find this incredibly helpful so do i do
[53:29.56]  apologize or do request in advance that i may send out a uh random
[53:35.50]  survey uh to folks to like a survey monkey or whatever type of thing to uh ask for advice as
[53:44.16]  we're building out some of the materials so that we're getting some advice from you folks along the
[53:48.02]  way um that would be very very helpful but aside from that i think i am at the end of what i wanted
[53:55.56]  to talk about today unless there's anything that i have forgotten about no
[54:00.06]  uh and any last
[54:05.50]  remarks that people want to say
[54:07.60]  any famous last words
[54:08.84]  any famous last words i'm also empathetic because it's now 1 15 for avon
[54:13.40]  uh so thank you avon uh for for uh staying up late and i'm hoping um well then
[54:26.48]  yeah oh yeah i am tripping over my words i'll simply say thank you for the fabulous feedback
[54:34.48]  and engagement
[54:35.50]  as always that i get with this group um and uh i'm looking forward to when we meet in april we'll
[54:43.12]  re-say what um uh opportunities and what shaping we're doing for the session in may uh when we meet
[54:53.86]  in april okay cool so with that have a good rest of your days be it you know early in the morning
[55:00.84]  or late at night thank you everyone thank you
[55:05.50]  bye thank you bye thank you bye bye thank you bye bye thank you happy new year